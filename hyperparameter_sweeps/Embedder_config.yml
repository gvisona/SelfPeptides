method: random
metric:
  goal: minimize
  name: train/loss
parameters:
  embedding_dim:
    value: 512
  projection_hidden_dim:
    value: 2048
  projection_dim:
    values: 
    - 16
    - 32
  PMA_num_heads:
    value: 1
  PMA_ln:
    value: True
  num_heads:
    value: 1
  transf_hidden_dim:
    value: 2048
  n_attention_layers:
    value: 2
  accumulate_batches:
    value: 1
  batch_size:
    value: 32
  cool_down:
    value: 0.6
  experiment_group:
    value: SP_Embedder
  experiment_name:
    value: Embeddings_CMT_long
  lr:
    distribution: log_uniform_values 
    max: 5.0e-6
    min: 1e-7
  momentum:
    values: 
    - 0.9
    - 0.8
  nesterov_momentum:
    value: True
  max_updates:
    value: 100000000
  min_frac:
    value: 0.1
  dropout_p:
    value: 0.0 
  project_folder:
    value: /fast/gvisona/SelfPeptides
  ramp_up:
    value: 0.3
  seed:
    value: 0
  val_size:
    value: 5000
  ref_size:
    value: 10000
  validate_every_n_updates:
    value: 50000
  weight_decay:
    value: 0.000001
  early_stopping:
    value: True
  patience:
    value: 500
  hdf5_dataset:
    value: "/home/gvisona/SelfPeptides/processed_data/Self_nonSelf/pre_tokenized_peptides_dataset.hdf5"
  reg_weight:
    value: 1e-4
  margin:
    value: 0.6
  loss_s:
    values: 
    - 1.0
    - 5.0