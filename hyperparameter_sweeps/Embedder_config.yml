method: random
metric:
  goal: minimize
  name: train/loss
parameters:
  embedding_dim:
    value: 512
  projection_hidden_dim:
    value: 2048
  projection_dim:
    value: 32
  PMA_num_heads:
    value: 1
  PMA_ln:
    value: True
  num_heads:
    values:
    - 4
    - 8
  transf_hidden_dim:
    value: 2048
  n_attention_layers:
    values: 
    - 1
    - 2
    - 4
  accumulate_batches:
    value: 1
  batch_size:
    value: 32
  cool_down:
    value: 0.6
  experiment_group:
    value: SP_Embedder
  experiment_name:
    value: Embeddings_CMT_meanKNN
  lr:
    distribution: log_uniform_values 
    max: 1.0e-5
    min: 1.0e-8
  momentum:
    value: 0.8
  nesterov_momentum:
    value: True
  max_updates:
    value: 10000000
  min_frac:
    value: 0.1
  dropout_p:
    value: 0.05
  project_folder:
    value: /fast/gvisona/SelfPeptides
  ramp_up:
    value: 0.3
  seed:
    distribution: int_uniform
    min: 0
    max: 100000
  val_size:
    value: 5000
  test_size:
    value: 5000
  ref_size:
    value: 10000
  validate_every_n_updates:
    value: 50000
  weight_decay:
    values: 
    - 0.0001
    - 0.00001
  early_stopping:
    value: True
  test_run:
    value: False
  patience:
    value: 1000
  hdf5_dataset:
    value: "/home/gvisona/SelfPeptides/processed_data/Self_nonSelf/pre_tokenized_peptides_dataset.hdf5"
  reg_weight:
    value: 1e-4
  margin:
    value: 0.6
  loss_s:
    value: 20.0
  pretrained_aa_embeddings:
    value: "/home/gvisona/SelfPeptides/processed_data/aa_embeddings/learned_BA_AA_embeddings.npy"