{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0cabde-6a66-4bb4-bc1e-64f0b5596f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 16:40:27.771376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-30 16:40:27.956514: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-30 16:40:27.993603: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-30 16:40:27.993641: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-01-30 16:40:28.573770: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-30 16:40:28.573819: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-30 16:40:28.573823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741d0858-ba73-4935-a36b-5df1757f7420",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a50f696-b0c5-468e-8cd2-b476efa50b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<cls>': 0,\n",
       " '<pad>': 1,\n",
       " '<eos>': 2,\n",
       " '<unk>': 3,\n",
       " 'L': 4,\n",
       " 'A': 5,\n",
       " 'G': 6,\n",
       " 'V': 7,\n",
       " 'S': 8,\n",
       " 'E': 9,\n",
       " 'R': 10,\n",
       " 'T': 11,\n",
       " 'I': 12,\n",
       " 'D': 13,\n",
       " 'P': 14,\n",
       " 'K': 15,\n",
       " 'Q': 16,\n",
       " 'N': 17,\n",
       " 'F': 18,\n",
       " 'Y': 19,\n",
       " 'M': 20,\n",
       " 'H': 21,\n",
       " 'W': 22,\n",
       " 'C': 23,\n",
       " 'X': 24,\n",
       " 'B': 25,\n",
       " 'U': 26,\n",
       " 'Z': 27,\n",
       " 'O': 28,\n",
       " '.': 29,\n",
       " '-': 30,\n",
       " '<null_1>': 31,\n",
       " '<mask>': 32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf831620-9e41-40f9-83cb-e00dacde95bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 480, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 480, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (key): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (value): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=480, out_features=1920, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1920, out_features=480, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=240, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=480, out_features=480, bias=True)\n",
       "    (layer_norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=480, out_features=33, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26fc24ee-c0f6-4f63-94f9-1c9414d023f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptide</th>\n",
       "      <th>HLA</th>\n",
       "      <th>Qualitative Measurement</th>\n",
       "      <th>Peptide length</th>\n",
       "      <th>Number of Subjects Tested</th>\n",
       "      <th>Number of Subjects Positive</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CYTWNQMNL</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>84.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GYDQIMPKK</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YLSGANLNL</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QYSWFVNGTF</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>Positive</td>\n",
       "      <td>10</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TYACFVSNL</td>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28295</th>\n",
       "      <td>RLLEQKVEL</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive-Low</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28296</th>\n",
       "      <td>HLVDEAHCLRL</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive-Low</td>\n",
       "      <td>11</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>QLVDIIEKV</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28298</th>\n",
       "      <td>ALSNLEVKL</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28299</th>\n",
       "      <td>TLFDYEVRL</td>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>Positive-Low</td>\n",
       "      <td>9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28300 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Peptide         HLA Qualitative Measurement  Peptide length  \\\n",
       "0        CYTWNQMNL  HLA-A24:02                Positive               9   \n",
       "1        GYDQIMPKK  HLA-A24:02                Positive               9   \n",
       "2        YLSGANLNL  HLA-A02:01                Positive               9   \n",
       "3       QYSWFVNGTF  HLA-A24:02                Positive              10   \n",
       "4        TYACFVSNL  HLA-A24:02                Positive               9   \n",
       "...            ...         ...                     ...             ...   \n",
       "28295    RLLEQKVEL  HLA-A02:01            Positive-Low               9   \n",
       "28296  HLVDEAHCLRL  HLA-A02:01            Positive-Low              11   \n",
       "28297    QLVDIIEKV  HLA-A02:01                Positive               9   \n",
       "28298    ALSNLEVKL  HLA-A02:01                Positive               9   \n",
       "28299    TLFDYEVRL  HLA-A02:01            Positive-Low               9   \n",
       "\n",
       "       Number of Subjects Tested  Number of Subjects Positive  Alpha  Beta  \n",
       "0                           84.0                         60.0   61.0  25.0  \n",
       "1                            2.0                          2.0    3.0   1.0  \n",
       "2                            8.0                          6.0    7.0   3.0  \n",
       "3                           12.0                          5.0    6.0   8.0  \n",
       "4                           12.0                          5.0    6.0   8.0  \n",
       "...                          ...                          ...    ...   ...  \n",
       "28295                       12.0                          3.0    4.0  10.0  \n",
       "28296                       12.0                          3.0    4.0  10.0  \n",
       "28297                       12.0                          6.0    7.0   7.0  \n",
       "28298                       12.0                          5.0    6.0   8.0  \n",
       "28299                       12.0                          3.0    4.0  10.0  \n",
       "\n",
       "[28300 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/gvisona/Projects/SelfPeptides/processed_data/Immunogenicity/Processed_TCell_IEDB_beta_summed.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c695c7fb-72bd-4a7f-8296-ecea1c527c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides = df[\"Peptide\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6be3934-e5e6-407d-a904-0777ef6b1851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CYTWNQMNL',\n",
       " 'GYDQIMPKK',\n",
       " 'YLSGANLNL',\n",
       " 'QYSWFVNGTF',\n",
       " 'TYACFVSNL',\n",
       " 'LMLGEFLKL',\n",
       " 'AAAAAIFVI',\n",
       " 'FLPSDFFPSV']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = df[\"Peptide\"].iloc[:8].tolist()\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ffab5bc-52c0-41ec-be36-8a2845a0d864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 23, 19, 11, 22, 17, 16, 20, 17,  4,  2,  1],\n",
       "        [ 0,  6, 19, 13, 16, 12, 20, 14, 15, 15,  2,  1],\n",
       "        [ 0, 19,  4,  8,  6,  5, 17,  4, 17,  4,  2,  1],\n",
       "        [ 0, 16, 19,  8, 22, 18,  7, 17,  6, 11, 18,  2],\n",
       "        [ 0, 11, 19,  5, 23, 18,  7,  8, 17,  4,  2,  1],\n",
       "        [ 0,  4, 20,  4,  6,  9, 18,  4, 15,  4,  2,  1],\n",
       "        [ 0,  5,  5,  5,  5,  5, 12, 18,  7, 12,  2,  1],\n",
       "        [ 0, 18,  4, 14,  8, 13, 18, 18, 14,  8,  7,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9eea057-6332-4c7f-98b5-c7f47f1dbbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 11, 11, 12, 11, 11, 11, 12])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = inputs[\"attention_mask\"].sum(axis=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30dd59d0-edee-49aa-936d-1c187cf9a366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8,  5,  9,  0,  4,  7,  0,  4,  5,  2,  8,  8,  9,  9,  5])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([np.random.choice(list(range(a[i])), size=2, replace=False) for i in range(len(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbe31a5d-4719-49db-a192-52135559a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d18e599c-28df-4bef-b7ca-9ca1f6ee0a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokenized_inputs(tokenized_dict, mlm_fraction=0.15, mask_token_id=32):\n",
    "    out_dict = deepcopy(tokenized_dict.copy())\n",
    "    lengths = tokenized_dict[\"attention_mask\"].sum(axis=1)\n",
    "    \n",
    "    row_ixs = []\n",
    "    col_ixs = []\n",
    "    \n",
    "    for i, l in enumerate(lengths):\n",
    "        n_masked_tokens = int(mlm_fraction*l)\n",
    "        row_ixs.extend([i]*n_masked_tokens)\n",
    "        col_ixs.append(np.random.choice(list(range(l)), size=n_masked_tokens, replace=False))\n",
    "    col_ixs = np.concatenate(col_ixs)\n",
    "    row_ixs = np.array(row_ixs)\n",
    "    out_dict[\"input_ids\"][row_ixs, col_ixs] = mask_token_id\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ede5163-392b-4217-85fa-c0ae1be0d164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 23, 19, 11, 22, 32, 16, 20, 17,  4,  2,  1],\n",
       "        [32,  6, 19, 13, 16, 12, 20, 14, 15, 15,  2,  1],\n",
       "        [ 0, 19,  4,  8,  6,  5, 17,  4, 17,  4, 32,  1],\n",
       "        [ 0, 16, 32,  8, 22, 18,  7, 17,  6, 11, 18,  2],\n",
       "        [ 0, 11, 19,  5, 23, 18, 32,  8, 17,  4,  2,  1],\n",
       "        [ 0,  4, 20,  4,  6, 32, 18,  4, 15,  4,  2,  1],\n",
       "        [ 0,  5,  5,  5,  5,  5, 12, 18,  7, 32,  2,  1],\n",
       "        [ 0, 18,  4, 14,  8, 13, 18, 18, 14,  8,  7, 32]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_inputs = mask_tokenized_inputs(inputs, mask_token_id=tokenizer.mask_token_id)\n",
    "masked_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c77e6edd-377f-47cf-8b0a-7506ddd0b8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 23, 19, 11, 22, 17, 16, 20, 17,  4,  2,  1],\n",
       "        [ 0,  6, 19, 13, 16, 12, 20, 14, 15, 15,  2,  1],\n",
       "        [ 0, 19,  4,  8,  6,  5, 17,  4, 17,  4,  2,  1],\n",
       "        [ 0, 16, 19,  8, 22, 18,  7, 17,  6, 11, 18,  2],\n",
       "        [ 0, 11, 19,  5, 23, 18,  7,  8, 17,  4,  2,  1],\n",
       "        [ 0,  4, 20,  4,  6,  9, 18,  4, 15,  4,  2,  1],\n",
       "        [ 0,  5,  5,  5,  5,  5, 12, 18,  7, 12,  2,  1],\n",
       "        [ 0, 18,  4, 14,  8, 13, 18, 18, 14,  8,  7,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a014f8-ceb0-4447-ba5a-214ceb5ea5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 23, 19, 11, 22, 17, 16, 20, 17,  4,  2,  1],\n",
       "        [ 0,  6, 19, 13, 16, 12, 20, 14, 15, 15,  2,  1],\n",
       "        [ 0, 19,  4,  8,  6,  5, 17,  4, 17,  4,  2,  1],\n",
       "        [ 0, 16, 19,  8, 22, 18,  7, 17,  6, 11, 18,  2],\n",
       "        [ 0, 11, 19,  5, 23, 18,  7,  8, 17,  4,  2,  1],\n",
       "        [ 0,  4, 20,  4,  6,  9, 18,  4, 15,  4,  2,  1],\n",
       "        [ 0,  5,  5,  5,  5,  5, 12, 18,  7, 12,  2,  1],\n",
       "        [ 0, 18,  4, 14,  8, 13, 18, 18, 14,  8,  7,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs2 = inputs.copy()\n",
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09973ab8-8cef-4123-8732-21d73b80899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19319dba-fd6f-4da7-830e-8dbe58fe97ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 23, 19, 11, 22, 17, 32, 20, 17,  4,  2,  1],\n",
       "        [ 0,  6, 19, 13, 32, 12, 20, 14, 15, 15,  2,  1],\n",
       "        [ 0, 19, 32,  8,  6,  5, 17,  4, 17,  4,  2,  1],\n",
       "        [ 0, 16, 19,  8, 22, 18,  7, 32,  6, 11, 18,  2],\n",
       "        [ 0, 11, 19,  5, 23, 18,  7,  8, 17, 32,  2,  1],\n",
       "        [ 0,  4, 32,  4,  6,  9, 18,  4, 15,  4,  2,  1],\n",
       "        [ 0,  5,  5, 32,  5,  5, 12, 18,  7, 12,  2,  1],\n",
       "        [ 0, 18,  4, 14,  8, 13, 18, 18, 14, 32,  7,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    ix = np.random.randint(0, 12)\n",
    "    inputs2[\"input_ids\"][i,ix] = tokenizer.mask_token_id\n",
    "inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7386f504-acca-4409-8d32-72dfe9f5d8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 33])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dfc4f1b-0f20-429b-8993-5942559dc9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d48691d-f61e-4b9b-abfe-2295085ad219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100,   16, -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100,   16, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100, -100,    4, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100,   17, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100,    4, -100, -100],\n",
       "        [-100, -100,   20, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100,    5, -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100,    8, -100, -100]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "\n",
    "labels = tokenizer(batch, return_tensors=\"pt\",padding=True)[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37bffaed-4830-4ede-aaf4-d24b9fcae9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7c3db3a-2fae-4d82-93ee-0c1f1b99dfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e626e648-e310-45df-908a-cd55ccb528a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=tensor(2.7584, grad_fn=<NllLossBackward0>), logits=tensor([[[ 13.8696,  -8.6695,  -0.3092,  ..., -15.3071, -15.1637,  -8.7102],\n",
       "         [ -6.7248, -12.8521, -10.0629,  ..., -14.8430, -14.6449, -12.8491],\n",
       "         [ -8.7960, -15.3293,  -8.4501,  ..., -15.6858, -15.6968, -15.3290],\n",
       "         ...,\n",
       "         [ -8.3384, -13.9196,  -8.1586,  ..., -15.3413, -15.3934, -13.9166],\n",
       "         [ -1.8464,  -9.7893,  13.3795,  ..., -15.7441, -15.9714,  -9.7985],\n",
       "         [ -9.0361, -14.4618,  -8.2976,  ..., -15.7336, -15.7147, -14.4684]],\n",
       "\n",
       "        [[ 11.7932, -11.1072,  -2.3654,  ..., -15.4867, -15.2818, -11.1253],\n",
       "         [ -8.6423, -15.4961,  -9.9237,  ..., -15.7949, -15.6557, -15.4870],\n",
       "         [ -8.3243, -14.2958,  -8.0217,  ..., -15.6748, -15.6683, -14.2933],\n",
       "         ...,\n",
       "         [ -8.8828, -14.9908,  -8.6261,  ..., -15.6530, -15.6450, -14.9874],\n",
       "         [ -2.7259, -10.9024,  12.5689,  ..., -15.8706, -16.1730, -10.8783],\n",
       "         [ -7.9415, -13.5158,  -7.1330,  ..., -15.6466, -15.6287, -13.5200]],\n",
       "\n",
       "        [[ 12.0095, -10.9299,  -1.8996,  ..., -15.3769, -15.1719, -10.9491],\n",
       "         [ -6.4309, -14.1246,  -8.0143,  ..., -15.2448, -15.1206, -14.1259],\n",
       "         [ -7.9513, -14.2922,  -7.7187,  ..., -15.5285, -15.4842, -14.2912],\n",
       "         ...,\n",
       "         [ -7.2432, -12.9356,  -6.8015,  ..., -15.1439, -15.1663, -12.9336],\n",
       "         [ -2.2299, -11.1479,  13.0820,  ..., -15.6945, -15.9359, -11.1382],\n",
       "         [ -7.6731, -13.2822,  -6.8117,  ..., -15.4843, -15.4413, -13.2883]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 12.6280, -10.3960,  -2.8179,  ..., -15.2089, -14.9977, -10.4125],\n",
       "         [ -5.3541, -13.2060,  -8.0556,  ..., -14.8389, -14.7723, -13.1975],\n",
       "         [ -7.2711, -14.2508,  -7.3798,  ..., -15.4741, -15.4520, -14.2525],\n",
       "         ...,\n",
       "         [ -6.5041, -12.9748,  -6.3615,  ..., -15.0679, -15.1039, -13.0011],\n",
       "         [ -2.3794,  -9.1527,  13.8605,  ..., -15.3229, -15.6762,  -9.1268],\n",
       "         [ -6.7889, -13.4790,  -6.4809,  ..., -15.2821, -15.2261, -13.4903]],\n",
       "\n",
       "        [[ 13.5613,  -8.3124,  -0.1936,  ..., -15.0777, -15.0127,  -8.3445],\n",
       "         [ -6.7513, -14.2198,  -9.3487,  ..., -15.6125, -15.2182, -14.2165],\n",
       "         [ -8.5400, -15.2876,  -8.4907,  ..., -15.8619, -15.6004, -15.2865],\n",
       "         ...,\n",
       "         [ -8.6521, -15.5846,  -8.7067,  ..., -16.0469, -16.1050, -15.5820],\n",
       "         [ -1.8854, -11.0907,  12.0035,  ..., -15.5754, -15.8355, -11.0937],\n",
       "         [ -8.8418, -15.1805,  -8.3462,  ..., -15.7200, -15.7140, -15.1852]],\n",
       "\n",
       "        [[ 13.6090,  -9.7200,  -1.7071,  ..., -15.1020, -14.9886,  -9.7425],\n",
       "         [ -5.8471, -13.4449,  -7.6362,  ..., -15.3180, -15.4550, -13.4421],\n",
       "         [ -7.2593, -13.7186,  -7.2975,  ..., -14.9593, -14.9952, -13.7122],\n",
       "         ...,\n",
       "         [ -7.8901, -14.3765,  -7.5278,  ..., -15.5232, -15.5406, -14.3760],\n",
       "         [ -6.9000, -13.3403,  -6.8724,  ..., -15.3251, -15.3383, -13.3347],\n",
       "         [ -0.9218,  -9.7881,  14.8629,  ..., -15.5342, -15.8503,  -9.7804]]],\n",
       "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, labels=labels, output_hidden_states=False)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66b1a2ea-dbf4-4565-bc2e-5661e8456282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a775ef5-63ea-47bf-bfef-4773bb5408f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 12, 480])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7216062e-0ee6-4d0a-a8cc-c85d66442dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d236425-4754-4e4b-9a47-5be1f17f1cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2fb3b811-f812-4f60-8bb8-d6e7b48ec02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[ 0, 23, 19, 11, 22, 17, 16, 20, 17,  4,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[ 0,  6, 19, 13, 16, 12, 20, 14, 15, 15,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " {'input_ids': tensor([[ 0, 19,  4,  8,  6,  5, 17,  4, 17,  4,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = [tokenizer(p, return_tensors=\"pt\", padding=True) for p in peptides]\n",
    "samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb81a794-302a-4cc5-acc9-7660e030871a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:717\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 717\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 11 at dim 2 (got 12)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeptide\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer\u001b[38;5;241m.\u001b[39mdecode(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/data/data_collator.py:45\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[0;34m(self, features, return_tensors)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/data/data_collator.py:731\u001b[0m, in \u001b[0;36mDataCollatorForLanguageModeling.torch_call\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtorch_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, examples: List[Union[List[\u001b[38;5;28mint\u001b[39m], Any, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;66;03m# Handle dict or lists with proper padding and conversion to tensor.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[0;32m--> 731\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    733\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: _torch_collate_batch(examples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer, pad_to_multiple_of\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_to_multiple_of)\n\u001b[1;32m    735\u001b[0m         }\n",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3020\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3017\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3018\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:210\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    206\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/proteinbert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:733\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    730\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m             )\n\u001b[0;32m--> 733\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    734\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m         )\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`input_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "for chunk in data_collator(samples)[\"peptide\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "983c81ad-0bed-45c7-996c-99fb6d6b2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/gvisona/Projects/SelfPeptides/processed_data/Immunogenicity/Processed_TCell_IEDB_beta_summed.csv\")\n",
    "ba_df = pd.read_csv(\"/home/gvisona/Projects/SelfPeptides/processed_data/Binding_Affinity/DHLAP_binding_affinity_data.csv\")\n",
    "dhlap_df = pd.read_csv(\"/home/gvisona/Projects/SelfPeptides/processed_data/Immunogenicity/DHLAP_immunogenicity_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cd01e6c-2759-4c50-84b2-6f8045dc8655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HLA</th>\n",
       "      <th>Peptide</th>\n",
       "      <th>Method</th>\n",
       "      <th>Measurement</th>\n",
       "      <th>Type</th>\n",
       "      <th>Affinity(nM)</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HLA-B08:01</td>\n",
       "      <td>HMMVIFRLM</td>\n",
       "      <td>purified MHC/competitive/fluorescence</td>\n",
       "      <td>half maximal inhibitory concentration (IC50)</td>\n",
       "      <td>Negative</td>\n",
       "      <td>3439140.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HLA-B08:01</td>\n",
       "      <td>NFLIKFLLI</td>\n",
       "      <td>purified MHC/competitive/fluorescence</td>\n",
       "      <td>half maximal inhibitory concentration (IC50)</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1606610.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HLA-A24:02</td>\n",
       "      <td>SSKYYIKNI</td>\n",
       "      <td>purified MHC/competitive/fluorescence</td>\n",
       "      <td>half maximal inhibitory concentration (IC50)</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1449300.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HLA-A11:01</td>\n",
       "      <td>KLYKNKSKQ</td>\n",
       "      <td>purified MHC/competitive/fluorescence</td>\n",
       "      <td>half maximal inhibitory concentration (IC50)</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1360000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HLA-A02:01</td>\n",
       "      <td>AMEKSSKYY</td>\n",
       "      <td>purified MHC/competitive/fluorescence</td>\n",
       "      <td>half maximal inhibitory concentration (IC50)</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1166430.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327173</th>\n",
       "      <td>HLA-A11:01</td>\n",
       "      <td>SVQMTLSK</td>\n",
       "      <td>purified MHC</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327174</th>\n",
       "      <td>HLA-A02:09</td>\n",
       "      <td>TLGIVCPI</td>\n",
       "      <td>cellular MHC/direct/fluorescence</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327175</th>\n",
       "      <td>HLA-A11:01</td>\n",
       "      <td>TTSPIPLK</td>\n",
       "      <td>purified MHC</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327176</th>\n",
       "      <td>HLA-A03:01</td>\n",
       "      <td>TVTTITVY</td>\n",
       "      <td>cellular MHC/direct/fluorescence</td>\n",
       "      <td>qualitative binding</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327177</th>\n",
       "      <td>HLA-B44:03</td>\n",
       "      <td>YEGLLDYW</td>\n",
       "      <td>secreted MHC/mass spectrometry</td>\n",
       "      <td>ligand presentation</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327178 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               HLA    Peptide                                 Method  \\\n",
       "0       HLA-B08:01  HMMVIFRLM  purified MHC/competitive/fluorescence   \n",
       "1       HLA-B08:01  NFLIKFLLI  purified MHC/competitive/fluorescence   \n",
       "2       HLA-A24:02  SSKYYIKNI  purified MHC/competitive/fluorescence   \n",
       "3       HLA-A11:01  KLYKNKSKQ  purified MHC/competitive/fluorescence   \n",
       "4       HLA-A02:01  AMEKSSKYY  purified MHC/competitive/fluorescence   \n",
       "...            ...        ...                                    ...   \n",
       "327173  HLA-A11:01   SVQMTLSK                           purified MHC   \n",
       "327174  HLA-A02:09   TLGIVCPI       cellular MHC/direct/fluorescence   \n",
       "327175  HLA-A11:01   TTSPIPLK                           purified MHC   \n",
       "327176  HLA-A03:01   TVTTITVY       cellular MHC/direct/fluorescence   \n",
       "327177  HLA-B44:03   YEGLLDYW         secreted MHC/mass spectrometry   \n",
       "\n",
       "                                         Measurement      Type  Affinity(nM)  \\\n",
       "0       half maximal inhibitory concentration (IC50)  Negative     3439140.0   \n",
       "1       half maximal inhibitory concentration (IC50)  Negative     1606610.0   \n",
       "2       half maximal inhibitory concentration (IC50)  Negative     1449300.0   \n",
       "3       half maximal inhibitory concentration (IC50)  Negative     1360000.0   \n",
       "4       half maximal inhibitory concentration (IC50)  Negative     1166430.0   \n",
       "...                                              ...       ...           ...   \n",
       "327173                           qualitative binding  Positive           0.0   \n",
       "327174                           qualitative binding  Positive           0.0   \n",
       "327175                           qualitative binding  Positive           0.0   \n",
       "327176                           qualitative binding  Positive           0.0   \n",
       "327177                           ligand presentation  Positive           0.0   \n",
       "\n",
       "        Label  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "327173      1  \n",
       "327174      1  \n",
       "327175      1  \n",
       "327176      1  \n",
       "327177      1  \n",
       "\n",
       "[327178 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df345a38-aabe-4266-adc7-e1d6c1fbbf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptide</th>\n",
       "      <th>HLA</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KLFNEHIEAL</td>\n",
       "      <td>HLA-A11:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KLFNEHIEAL</td>\n",
       "      <td>HLA-B40:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KLFNEHIEAL</td>\n",
       "      <td>HLA-B58:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KLFNEHIEAL</td>\n",
       "      <td>HLA-C02:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KLFNEHIEAL</td>\n",
       "      <td>HLA-C07:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724635</th>\n",
       "      <td>IAVDVTSAL</td>\n",
       "      <td>HLA-A11:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724636</th>\n",
       "      <td>IAVDVTSAL</td>\n",
       "      <td>HLA-B15:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724637</th>\n",
       "      <td>IAVDVTSAL</td>\n",
       "      <td>HLA-B35:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724638</th>\n",
       "      <td>IAVDVTSAL</td>\n",
       "      <td>HLA-C03:03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724639</th>\n",
       "      <td>IAVDVTSAL</td>\n",
       "      <td>HLA-C04:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Peptide         HLA  Label\n",
       "0       KLFNEHIEAL  HLA-A11:01      0\n",
       "1       KLFNEHIEAL  HLA-B40:02      0\n",
       "2       KLFNEHIEAL  HLA-B58:01      0\n",
       "3       KLFNEHIEAL  HLA-C02:02      0\n",
       "4       KLFNEHIEAL  HLA-C07:01      0\n",
       "...            ...         ...    ...\n",
       "724635   IAVDVTSAL  HLA-A11:01      0\n",
       "724636   IAVDVTSAL  HLA-B15:01      0\n",
       "724637   IAVDVTSAL  HLA-B35:01      1\n",
       "724638   IAVDVTSAL  HLA-C03:03      1\n",
       "724639   IAVDVTSAL  HLA-C04:01      1\n",
       "\n",
       "[724640 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "la_df = pd.read_csv(\"/home/gvisona/Projects/SelfPeptides/processed_data/Binding_Affinity/HLA_Ligand_Atlas_processed.csv\")\n",
    "la_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "393c0b89-3734-499f-8ce6-de11aea7f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KLFNEHIEAL', 'SRVEAVYVL', 'GEDPRVSINV']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peptides = la_df[\"Peptide\"].unique().tolist()\n",
    "peptides[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a501585-2466-464e-a98f-597308f2d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2308318d-ac52-41c8-880a-98ca2677e23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PeptidesDataset at 0x7fc1d905ab00>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = PeptidesDataset(peptides)\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00f5b51b-fa95-4f1a-a0c2-47dddef0ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeptidesDataset(Dataset):\n",
    "    def __init__(self, peptides):\n",
    "        super().__init__()\n",
    "        self.peptides = peptides\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.peptides)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        return self.peptides[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fa6f16c-4e21-4f6e-b805-f3cf4d4c6406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KLFNEHIEAL'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b4ccf-7287-45fa-a122-9e831c787729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
